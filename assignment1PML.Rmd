---
title: "Study on Human Activity Recognition"
author: "Sofia Cividini"
date: "Friday, November 21, 2014"
output:
  html_document:
    keep_md: yes
---

### Synopsis.

Thanks to new technological devices it is now possible to collect a large amount of data about personal activity without an excess of costs. Through these devices, people are able to collect by themselves a great quantity of data about their physical activity in order to improve their health, to find patterns in their behavior, or because they are tech geeks. 
In general, people are more concentrated on quantifying <strong>how much</strong> of a particular activity they do, rather than quantifying <strong>how well</strong> they do that activity. In this project, the goal was to create a model which was able to classify data from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants, who were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. 
The overall data set consisted of 19622 observations and 160 predictors. After an opportune action of data cleaning, I got a data set with 19622 observations and 53 predictors. I subdivided this data set into other two data sets, one with the 60% of observations and the other one with the 40% of observations. Then, I applied a model for the classification which uses an algorithm based on the <strong>random forests </strong>. 

```{r,echo=FALSE}
setwd("C:/Users/Sofia Cividini/Documents/COURSERA COURSES/Data Science Specialization_JHU/Practical Machine Learning/Assignment1_Write up")
```
```{r, echo=TRUE}
pml.training <- read.csv("C:/Users/Sofia Cividini/Documents/COURSERA COURSES/Data Science Specialization_JHU/Practical Machine Learning/Assignment1_Write up/pml-training.csv", header=TRUE, sep=",")
dim(pml.training)
```

```{r,echo=TRUE}
library(caret)
library(randomForest)
```

* I deleted all the columns where the data are almost all missing. This way I reduced the data set dimension at 19622 observations and 60 predictors.

```{r, echo=TRUE}
pml.training2 <- pml.training[,-c(12:36, 50:59, 69:83, 87:101, 103:112, 125:139, 141:150)]
# pml.training2 <- edit(pml.training2) 
# attributes(pml.training2)
dim(pml.training2)
```

* Then, I deleted all the other variables which were not to be considered, and I held only the predictors corresponding to belt, forearm, arm, and dumbbell. This way I reduced the data set dimension at 19622 observations and 53 predictors.

```{r, echo=TRUE}
pml.training3 <- pml.training2[,-c(1:7)]
# pml.training3 <- edit(pml.training3) 
dim(pml.training3)
```

#### Exploratory Analysis.

I did a plot to see the frequency of the 5 different levels of the "classe" variable. The highest frequency is equivalent to the level A, while the lowest frequency is equivalent to the level D.

```{r, echo=TRUE, fig.height=5, fig.width=8}
plot <- ggplot(pml.training3, aes(x=classe, fill=classe)) + geom_histogram()
plot 
```


#### Model with an Algorithm based on Random Forests .

* I subdivided the data set called pml.training3 in a training data set (60% of observations) and in a test dataset (40% of observations). 

```{r,echo=TRUE}
inTrain <- createDataPartition(y=pml.training3$classe, p=0.6, list=FALSE)
training <- pml.training3
training <- pml.training3[inTrain,]
testing <- pml.training3[-inTrain,]
dim(training); dim(testing)
```

* I used the training data set (with the 60% of observations) in order to fit my model through randomForest() function. The model has an OOB estimate of the error rate rather good, and it is able to classify in a correct way most of the observations in the data set. In fact, as you can see from the confusion matrix, most of the observations are on the main diagonal.


```{r, echo=TRUE}
set.seed(12345)
modFit <- randomForest(classe ~ ., data=training, proximity=TRUE, importance=TRUE)
print(modFit)
```

* This plot shows the trend of the error along the 500 trees which were considered by the model.

```{r, echo=TRUE, fig.height=7, fig.width=10}
plot(modFit, log="y")
```

* I also took a look at the most important variables. As we can see in the following graph, there are 30 predictors out of 53 used which are important for the classification.

```{r,echo=TRUE, fig.height=8, fig.width=12}
varImpPlot(modFit)
```

* I did a prediction on the testing data set with the 40% of observations.

```{r,echo=TRUE}
prediction <- predict(modFit, testing)
testing$predRight <- prediction==testing$classe
table(prediction, testing$classe)
```


It seems that the model worked very well even in prediction on the testing data set as we can see from the confusion matrix.

